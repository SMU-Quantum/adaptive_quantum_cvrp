# cvrp_tripartite_solver/src/rl_classical/penalty_networks.py

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.distributions import Normal
import numpy as np # Should be imported if used, though not directly in this snippet but good practice
from typing import Tuple # For type hinting

# Constants for network architecture and action scaling
N_INSTANCE_FEATURES = 6  # Ensure this matches the features from AlmPenaltyEnv

# Define penalty bounds directly in this file for the network's output scaling logic
MIN_PENALTY_RHO = 0.1
MAX_PENALTY_RHO = 1000.0
MIN_PENALTY_SIGMA = 0.1
MAX_PENALTY_SIGMA = 1000.0

LOG_SIG_MAX = 2
LOG_SIG_MIN = -20

# Action scaling parameters
ACTION_SCALE_RHO = (MAX_PENALTY_RHO - MIN_PENALTY_RHO) / 2.0
ACTION_BIAS_RHO = (MAX_PENALTY_RHO + MIN_PENALTY_RHO) / 2.0
ACTION_SCALE_SIGMA = (MAX_PENALTY_SIGMA - MIN_PENALTY_SIGMA) / 2.0
ACTION_BIAS_SIGMA = (MAX_PENALTY_SIGMA + MIN_PENALTY_SIGMA) / 2.0


class Actor(nn.Module):
    """
    Policy Network (Actor) for SAC.
    Outputs parameters for a squashed Gaussian distribution over actions.
    Action space is 2D: [rho_penalty, sigma_penalty]
    """
    def __init__(self, state_dim: int = N_INSTANCE_FEATURES, action_dim: int = 2, hidden_dim: int = 256):
        super(Actor, self).__init__()
        self.fc1 = nn.Linear(state_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.mean_layer = nn.Linear(hidden_dim, action_dim)
        self.log_std_layer = nn.Linear(hidden_dim, action_dim)

    def forward(self, state: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        x = F.relu(self.fc1(state))
        x = F.relu(self.fc2(x))
        mean = self.mean_layer(x)
        log_std = self.log_std_layer(x)
        log_std = torch.clamp(log_std, LOG_SIG_MIN, LOG_SIG_MAX)
        return mean, log_std

    def sample(self, state: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        mean, log_std = self.forward(state)
        std = log_std.exp()
        normal = Normal(mean, std)
        
        x_t = normal.rsample()
        y_t = torch.tanh(x_t)
        
        log_prob = normal.log_prob(x_t)
        log_prob -= torch.log((1 - y_t.pow(2)) + 1e-6)
        log_prob = log_prob.sum(dim=1, keepdim=True)

        # Rescale y_t from [-1, 1] to the actual action space bounds
        action_rho_scaled = y_t[:, 0:1] * ACTION_SCALE_RHO + ACTION_BIAS_RHO
        action_sigma_scaled = y_t[:, 1:2] * ACTION_SCALE_SIGMA + ACTION_BIAS_SIGMA
        
        # Clamp to ensure actions are strictly within defined penalty limits
        action_rho_clamped = torch.clamp(action_rho_scaled, MIN_PENALTY_RHO, MAX_PENALTY_RHO)
        action_sigma_clamped = torch.clamp(action_sigma_scaled, MIN_PENALTY_SIGMA, MAX_PENALTY_SIGMA)

        final_action = torch.cat([action_rho_clamped, action_sigma_clamped], dim=1)
        
        return final_action, log_prob, mean


class Critic(nn.Module):
    """
    Q-Value Network (Critic) for SAC.
    Takes state and action as input, outputs a Q-value.
    """
    def __init__(self, state_dim: int = N_INSTANCE_FEATURES, action_dim: int = 2, hidden_dim: int = 256):
        super(Critic, self).__init__()
        # Q1 architecture
        self.fc1_q1 = nn.Linear(state_dim + action_dim, hidden_dim)
        self.fc2_q1 = nn.Linear(hidden_dim, hidden_dim)
        self.fc3_q1 = nn.Linear(hidden_dim, 1)

        # Q2 architecture
        self.fc1_q2 = nn.Linear(state_dim + action_dim, hidden_dim)
        self.fc2_q2 = nn.Linear(hidden_dim, hidden_dim)
        self.fc3_q2 = nn.Linear(hidden_dim, 1)

    def forward(self, state: torch.Tensor, action: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        sa = torch.cat([state, action], 1)

        q1 = F.relu(self.fc1_q1(sa))
        q1 = F.relu(self.fc2_q1(q1))
        q1 = self.fc3_q1(q1)

        q2 = F.relu(self.fc1_q2(sa))
        q2 = F.relu(self.fc2_q2(q2))
        q2 = self.fc3_q2(q2)
        return q1, q2

if __name__ == '__main__':
    batch_size = 4
    state_dim = N_INSTANCE_FEATURES
    action_dim = 2

    dummy_state = torch.randn(batch_size, state_dim)

    actor_net = Actor(state_dim, action_dim)
    actions, log_probs, means = actor_net.sample(dummy_state)
    print("--- Actor Test ---")
    print(f"Input state shape: {dummy_state.shape}")
    print(f"Sampled actions shape: {actions.shape}")
    print(f"Sampled actions:\n{actions}")
    print(f"Log probabilities shape: {log_probs.shape}")
    print(f"Means shape: {means.shape}")

    assert torch.all(actions[:, 0] >= MIN_PENALTY_RHO) and torch.all(actions[:, 0] <= MAX_PENALTY_RHO)
    assert torch.all(actions[:, 1] >= MIN_PENALTY_SIGMA) and torch.all(actions[:, 1] <= MAX_PENALTY_SIGMA)

    critic_net = Critic(state_dim, action_dim)
    # Use the actions generated by the actor (which are now in the correct scale)
    # or generate dummy actions within the expected scaled range.
    dummy_actions_for_critic = actions.detach() # Use actions from actor or create new ones in range
    
    q1_values, q2_values = critic_net(dummy_state, dummy_actions_for_critic)
    print("\n--- Critic Test ---")
    print(f"Input state shape: {dummy_state.shape}")
    print(f"Input actions shape: {dummy_actions_for_critic.shape}")
    print(f"Q1 values shape: {q1_values.shape}")
    print(f"Q2 values shape: {q2_values.shape}")

    print("\nNetwork definitions test run completed.")